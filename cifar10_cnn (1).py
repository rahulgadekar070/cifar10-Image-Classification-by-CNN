# -*- coding: utf-8 -*-
"""Cifar10_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107cKAcAT81J4m2OwmOG9emy-YaHrkHFN

"""
#Install Tensorflow 2.0
!pip install tensorflow==2.0.0
import tensorflow as tf
print(tf.__version__)

"""Import Packages"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.models import Model

# The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, 
# with 6000 images per class. There are 50000 training images and 10000 test images.

"""Load the Dataset"""
cfr10 = tf.keras.datasets.cifar10
(x_train,y_train),(x_test,y_test) = cfr10.load_data()

"""Check the shape of data..."""
print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)


"""**Pre-processing**"""
# Normalization on feature data & Categorize label data
x_train, x_test = x_train/255.0, x_test/255.0
#OR... tf.keras.utils.normalize(x_train, axis=1)

y_train, y_test = y_train.flatten(), y_test.flatten()   #for categorical convert into categorical format
# from keras.utils import to_categorical
#OR... y_train, y_test = to_categorical(y_train, classes), to_categorical(y_test, classes)

print(x_train[0])
print(y_train[3])
print(x_train[0].shape)    #(32,32,3) it shows image size(pixel)
print(x_train.shape[0])    #50000 it shows No. of images in x_train

"""Check Number of classes"""
k = len(set(y_train))
print("Number of classes:", k)

# Target classes
classes = 10


"""**Visualization**"""
fig, axes = plt.subplots(k, 10, figsize=(10, 10))
for i in range(k): 
    indice = np.where(y_train == i)[0] 
    for j in range(10): 
        axes[i][j].imshow(x_train[indice[j]])
        
        axes[i][j].set_xticks([]) 
        axes[i][j].set_yticks([])
plt.tight_layout()
plt.show()


"""# **Model Building**
**Model-1 with Sequential()...**
"""
model1 = tf.keras.Sequential()

model1.add(Conv2D(32, (3,3), padding='same', input_shape=x_train[0].shape, activation='relu'))
model1.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model1.add(MaxPooling2D(2,2))
model1.add(Dropout(0.25))

model1.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model1.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model1.add(MaxPooling2D(2,2))
model1.add(Dropout(0.25))

model1.add(Flatten())
model1.add(Dense(512, activation='relu'))
model1.add(Dropout(0.5))
model1.add(Dense(classes, activation='softmax'))


"""Compile the Model"""
model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model1.summary()


"""Fit the Model"""
hist1 = model1.fit(x_train,y_train, batch_size=32, epochs=15 , validation_data=(x_test,y_test))
# 258s 5ms/sample - loss: 0.5138 - accuracy: 0.8192 - val_loss: 0.6777 - val_accuracy: 0.7793


"""Accuracy"""
score = model1.evaluate(x_test, y_test)
print('Test loss:', score[0])
print('Test Accuracy:', score[1])

"""Loss Plot..."""
# Plot Loss per iteration...
plt.plot(hist1.history['loss'], label='loss')
plt.plot(hist1.history['val_loss'], label='val_loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc = 'upper right')

"""Accuracy Plot..."""
# Plot Accuracy per iteration...
plt.plot(hist1.history['accuracy'], label='accuracy')
plt.plot(hist1.history['val_accuracy'], label='val_acc')
plt.ylabel('model_acuracy')
plt.xlabel('epochs')
plt.legend(loc = 'upper left')


"""**Model-2 with BatchNormalization**"""
i = Input(shape=x_train[0].shape)   #shape 0f i/p layer

x = Conv2D(32, (3,3), activation='relu', padding='same')(i)
x = BatchNormalization()(x)
x = Conv2D(32, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Flatten()(x)
x = Dropout(0.2)(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)
x = Dense(k, activation='softmax')(x)

model2 = Model(i, x)


"""Compile the Model"""
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model2.summary()

"""Training the model by fitting..."""
fit = model2.fit(x_train,y_train, epochs=10, validation_data=(x_test,y_test))
# 441s 9ms/sample - loss: 0.2194 - accuracy: 0.9234 - val_loss: 0.6898 - val_accuracy: 0.8133

score = model2.evaluate(x_test, y_test)
print('test loss:', score[0])
print('Test Accuracy:', score[1])

"""Plot the Loss"""
# Plot Loss per iteration...
plt.plot(fit.history['loss'], label='loss')
plt.plot(fit.history['val_loss'], label='val_loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc = 'upper right')

"""Plot the Accuracy"""
# Plot Accuracy per iteration...
plt.plot(fit.history['accuracy'], label='accuracy')
plt.plot(fit.history['val_accuracy'], label='val_acc')
plt.ylabel('model_acuracy')
plt.xlabel('epochs')
plt.legend(loc = 'upper left')

"""Confusion Matrix"""
from sklearn.metrics import confusion_matrix
import itertools
plt.rcParams['figure.figsize'] = [10,7]

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
    print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")
      
  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()

p_test = model2.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

"""**Define the Labels**"""
labels = '''airplane, automobile, bird, cat, deer, 
dog, frog, horse,ship,truck''' .split()

"""Check the Right Predictions"""
misclassified_idx = np.where(p_test == y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (labels[y_test[i]], labels[p_test[i]]))

"""Check the Wrong predictions"""
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (labels[y_test[i]], labels[p_test[i]]))


"""**Retrain data with Augmentation** on Batch_Normalization Model"""
batch_size = 32
data_generator = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,
                                                                 height_shift_range=0.1,
                                                                 horizontal_flip=True,
                                                                 fill_mode='nearest')
train_generator = data_generator.flow(x_train,y_train,batch_size)
steps_per_epoch = x_train.shape[0] // batch_size
aug_fit = model2.fit_generator(train_generator, validation_data=(x_test,y_test),
steps_per_epoch=steps_per_epoch, epochs=10)
# 555s 355ms/step - loss: 0.3582 - accuracy: 0.8796 - val_loss: 0.4488 - val_accuracy: 0.8554


"""Plot the Loss"""
# Plot Loss per iteration...
plt.plot(aug_fit.history['loss'], label='loss')
plt.plot(aug_fit.history['val_loss'], label='val_loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc = 'upper right')

"""Plot the Accuracy"""
# Plot Accuracy per iteration...
plt.plot(aug_fit.history['accuracy'], label='accuracy')
plt.plot(aug_fit.history['val_accuracy'], label='val_acc')
plt.ylabel('model_acuracy')
plt.xlabel('epochs')
plt.legend(loc = 'upper left')

###### Comment:-
#1) by simple sequential deep learning model we got below results(epochs=15)
#   Train_Accuracy = 81.92%        Test_Accuracy = 78% 
#   Train_Loss     = 0.514         Test_Loss     = 0.677
#2) by Using Batch_Normalization Deep Neural Network we got,(epochs=10)
#   Train_Accuracy = 92.34%        Test_Accuracy = 81.53%
#   Train_Loss     = 0.2194        Test_Loss     = 0.6898
#3) by using Data Augmentation method on Batch Normalization model we got(epochs=10)
#   Train_Accuracy = 88%           Test_Accuracy = 85.54%
#   Train_Loss     = 0.3582        Test_Loss     = 0.4488
