# -*- coding: utf-8 -*-
"""Cifar10_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107cKAcAT81J4m2OwmOG9emy-YaHrkHFN

"""
#Install Tensorflow 2.0
!pip install tensorflow==2.0.0
import tensorflow as tf
print(tf.__version__)

"""Import Packages"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.models import Model

# The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, 
# with 6000 images per class. There are 50000 training images and 10000 test images.

"""Load the Dataset"""
cfr10 = tf.keras.datasets.cifar10
(x_train,y_train),(x_test,y_test) = cfr10.load_data()

"""Check the shape of data..."""
print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)
# (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)


"""**Pre-processing**"""
# Normalization on feature data & Categorize label data
x_train, x_test = x_train/255.0, x_test/255.0
#OR... tf.keras.utils.normalize(x_train, axis=1)

y_train, y_test = y_train.flatten(), y_test.flatten()   #for categorical convert into categorical format
# from keras.utils import to_categorical
#OR... y_train, y_test = to_categorical(y_train, classes), to_categorical(y_test, classes)

print(x_train[0])
print(y_train[3])
print(x_train[0].shape)    #(32,32,3) it shows image size(pixel)
print(x_train.shape[0])    #50000 it shows No. of images in x_train

"""Check Number of classes"""
k = len(set(y_train))
print("Number of classes:", k)

# Target classes
classes = 10


"""**Visualization**"""
fig, axes = plt.subplots(k, 10, figsize=(10, 10))
for i in range(k): 
    indice = np.where(y_train == i)[0] 
    for j in range(10): 
        axes[i][j].imshow(x_train[indice[j]])
        
        axes[i][j].set_xticks([]) 
        axes[i][j].set_yticks([])
plt.tight_layout()
plt.show()


""" **Model Building**
**Model-1 with Sequential()...**
"""
model1 = tf.keras.Sequential()

model1.add(Conv2D(32, (3,3), padding='same', input_shape=x_train[0].shape, activation='relu'))
model1.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model1.add(MaxPooling2D(2,2))
model1.add(Dropout(0.25))

model1.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model1.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model1.add(MaxPooling2D(2,2))
model1.add(Dropout(0.25))

model1.add(Flatten())
model1.add(Dense(512, activation='relu'))
model1.add(Dropout(0.5))
model1.add(Dense(classes, activation='softmax'))


"""Compile the Model"""
model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model1.summary()
# Model: "sequential"
_________________________________________________________________
#Layer (type)                 Output Shape              Param    
=================================================================
#conv2d (Conv2D)              (None, 32, 32, 32)        896       
_________________________________________________________________
#conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
#max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         
_________________________________________________________________
#dropout (Dropout)            (None, 16, 16, 32)        0         
_________________________________________________________________
#conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
#conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
#max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
#dropout_1 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
#flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
#dense (Dense)                (None, 512)               2097664   
_________________________________________________________________
#dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
#dense_1 (Dense)              (None, 10)                5130      
=================================================================
#Total params: 2,168,362
#Trainable params: 2,168,362
#Non-trainable params: 0
__________________________
##########################################################################################


"""Fit the Model"""
hist1 = model1.fit(x_train,y_train, batch_size=32, epochs=15 , validation_data=(x_test,y_test))
# 258s 5ms/sample - loss: 0.5138 - accuracy: 0.8192 - val_loss: 0.6777 - val_accuracy: 0.7793


"""Accuracy"""
score = model1.evaluate(x_test, y_test)
print('Test loss:', score[0])
print('Test Accuracy:', score[1])
# Test loss: 0.6776633263587951
# Test Accuracy: 0.7793


"""Loss Plot..."""
# Plot Loss per iteration...
plt.plot(hist1.history['loss'], label='loss')
plt.plot(hist1.history['val_loss'], label='val_loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc = 'upper right')

"""Accuracy Plot..."""
# Plot Accuracy per iteration...
plt.plot(hist1.history['accuracy'], label='accuracy')
plt.plot(hist1.history['val_accuracy'], label='val_acc')
plt.ylabel('model_acuracy')
plt.xlabel('epochs')
plt.legend(loc = 'upper left')
###############################################################################################################################

# we also try Batch Normalization deep neural network method,Becaause it reduces covariant shift,
# also it makes the input to each layer have zero mean, so it gives better test performance.

"""**Model-2 with BatchNormalization**"""

i = Input(shape=x_train[0].shape)   #shape 0f i/p layer

x = Conv2D(32, (3,3), activation='relu', padding='same')(i)
x = BatchNormalization()(x)
x = Conv2D(32, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Flatten()(x)
x = Dropout(0.2)(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)
x = Dense(k, activation='softmax')(x)

model2 = Model(i, x)


"""Compile the Model"""
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model2.summary()
# Model: "model"
_________________________________________________________________
#Layer (type)                 Output Shape              Param #   
=================================================================
#input_1 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
#conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       
_________________________________________________________________
#batch_normalization (BatchNo (None, 32, 32, 32)        128       
_________________________________________________________________
#conv2d_5 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
#batch_normalization_1 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
#max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
#conv2d_6 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
#batch_normalization_2 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
#conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
#batch_normalization_3 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
#max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
#conv2d_8 (Conv2D)            (None, 8, 8, 128)         73856     
_________________________________________________________________
#batch_normalization_4 (Batch (None, 8, 8, 128)         512       
_________________________________________________________________
#conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    
_________________________________________________________________
#batch_normalization_5 (Batch (None, 8, 8, 128)         512
#max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
#flatten_1 (Flatten)          (None, 2048)              0         
_________________________________________________________________
#dropout_3 (Dropout)          (None, 2048)              0         
_________________________________________________________________
#dense_2 (Dense)              (None, 1024)              2098176   
_________________________________________________________________
#dropout_4 (Dropout)          (None, 1024)              0         
_________________________________________________________________
#dense_3 (Dense)              (None, 10)                10250     
=================================================================
#Total params: 2,397,226
#Trainable params: 2,396,330
#Non-trainable params: 896
______________
###############################################################################################

"""Training the model by fitting..."""
fit = model2.fit(x_train,y_train, epochs=10, validation_data=(x_test,y_test))
# 441s 9ms/sample - loss: 0.2194 - accuracy: 0.9234 - val_loss: 0.6898 - val_accuracy: 0.8133

score = model2.evaluate(x_test, y_test)
print('test loss:', score[0])
print('Test Accuracy:', score[1])
# test loss: 0.6898217641353607
# Test Accuracy: 0.8133

"""Plot the Loss"""
# Plot Loss per iteration...
plt.plot(fit.history['loss'], label='loss')
plt.plot(fit.history['val_loss'], label='val_loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc = 'upper right')

"""Plot the Accuracy"""
# Plot Accuracy per iteration...
plt.plot(fit.history['accuracy'], label='accuracy')
plt.plot(fit.history['val_accuracy'], label='val_acc')
plt.ylabel('model_acuracy')
plt.xlabel('epochs')
plt.legend(loc = 'upper left')

"""Confusion Matrix"""
from sklearn.metrics import confusion_matrix
import itertools
plt.rcParams['figure.figsize'] = [10,7]

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
    print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")
      
  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()

p_test = model2.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))
# Confusion matrix, without normalization
# [[870   6  14  39   8   2   5   8  22  26]
# [ 14 893   1   3   1   2   8   0  19  59]
# [ 63   3 595 119  51  73  66  24   2   4]
# [ 13   2  17 773  25  95  37  23   5  10]
# [ 11   2  33  98 732  31  40  45   6   2]
# [  7   1  14 202  20 725   7  20   1   3]
# [  6   2  10  74  12  21 864   5   4   2]
# [  4   1   4  55  22  26   2 870   2  14]
# [ 46   6   3  11   1   1   5   5 901  21]
# [ 18  39   3  11   0   2   3   4  10 910]]
#########################################################################################

"""**Define the Labels**"""
labels = '''airplane, automobile, bird, cat, deer, 
dog, frog, horse,ship,truck''' .split()

"""Check the Right Predictions"""
misclassified_idx = np.where(p_test == y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (labels[y_test[i]], labels[p_test[i]]))
# Text(0.5, 1.0, 'True label: deer, Predicted: deer,')

"""Check the Wrong predictions"""
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (labels[y_test[i]], labels[p_test[i]]))
# Text(0.5, 1.0, 'True label: dog, Predicted: cat,')
#################################################################################################################################

# In Batch Normalization we see that it gives better training accuracy but lower test accuracy so it leads the overfitting
# problem so to avoid this we use Data Augmentation on this Batch Normalization model by retraining.

"""**Retrain data with Augmentation** on Batch_Normalization Model"""

batch_size = 32
data_generator = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,
                                                                 height_shift_range=0.1,
                                                                 horizontal_flip=True,
                                                                 fill_mode='nearest')
train_generator = data_generator.flow(x_train,y_train,batch_size)
steps_per_epoch = x_train.shape[0] // batch_size
aug_fit = model2.fit_generator(train_generator, validation_data=(x_test,y_test),
steps_per_epoch=steps_per_epoch, epochs=10)
# 555s 355ms/step - loss: 0.3582 - accuracy: 0.8796 - val_loss: 0.4488 - val_accuracy: 0.8554


"""Plot the Loss"""
# Plot Loss per iteration...
plt.plot(aug_fit.history['loss'], label='loss')
plt.plot(aug_fit.history['val_loss'], label='val_loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc = 'upper right')

"""Plot the Accuracy"""
# Plot Accuracy per iteration...
plt.plot(aug_fit.history['accuracy'], label='accuracy')
plt.plot(aug_fit.history['val_accuracy'], label='val_acc')
plt.ylabel('model_acuracy')
plt.xlabel('epochs')
plt.legend(loc = 'upper left')

###### Comment:-
#1) by simple sequential deep learning model we got below results(epochs=15)
#   Train_Accuracy = 81.92%        Test_Accuracy = 78% 
#   Train_Loss     = 0.514         Test_Loss     = 0.677
#2) by Using Batch_Normalization Deep Neural Network we got,(epochs=10)
#   Train_Accuracy = 92.34%        Test_Accuracy = 81.53%
#   Train_Loss     = 0.2194        Test_Loss     = 0.6898
#3) by using Data Augmentation method on Batch Normalization model we got(epochs=10)
#   Train_Accuracy = 88%           Test_Accuracy = 85.54%
#   Train_Loss     = 0.3582        Test_Loss     = 0.4488
  # from above results we can conclude that, Train Accuracy & Test Accuracy obtained by Data Augmentation method on Batch 
  # Normalization model are closer so there is no overfiiting problem which is usually saw in CNN model.
